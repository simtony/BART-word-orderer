{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, product\n",
    "from ptb_utils import parse_ptb_tree_data, partial_tree, linearize\n",
    "\n",
    "splits = [\"train\", \"dev\", \"test\"]\n",
    "mode2param = {\n",
    "    \"base\": {\"keep_pos\": 0, \"keep_dep\": 0, \"bracket\": False, \"rel\": False},\n",
    "    \"brac\": {\"keep_pos\": 0, \"keep_dep\": 0, \"bracket\": True, \"rel\": False},\n",
    "    \"pos\":  {\"keep_pos\": 1, \"keep_dep\": 0, \"bracket\": True, \"rel\": False},\n",
    "    \"udep\": {\"keep_pos\": 0, \"keep_dep\": 1, \"bracket\": True, \"rel\": False},\n",
    "    \"ldep\": {\"keep_pos\": 0, \"keep_dep\": 1, \"bracket\": True, \"rel\": True},\n",
    "    \"full\": {\"keep_pos\": 1, \"keep_dep\": 1, \"bracket\": True, \"rel\": True},\n",
    "}\n",
    "out = Path(\"data-raw\")\n",
    "data_path = Path(\"ptb_trees\")\n",
    "os.makedirs(out, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# load ptb trees\n",
    "data = dict()\n",
    "for split in splits:\n",
    "    data[split] = parse_ptb_tree_data(data_path / \"{}.txt\".format(split))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# update special tokens\n",
    "# they should be kept intact during tokenization\n",
    "pcount = Counter()\n",
    "rcount = Counter()\n",
    "specials = []\n",
    "for nodes in chain.from_iterable(data.values()):\n",
    "    pcount.update(node.pos for node in nodes)\n",
    "    rcount.update(node.rel for node in nodes)\n",
    "\n",
    "if not specials:\n",
    "    specials = [nodes[0].start, nodes[0].end]\n",
    "specials.extend(next(zip(*pcount.most_common())))\n",
    "specials.extend(next(zip(*rcount.most_common())))\n",
    "with open(out / \"new_tokens.txt\", \"w\") as fout:\n",
    "    for token in specials:\n",
    "        fout.write(\"{}\\n\".format(token))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Differet types of inputs share the same targets\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    with open(out / \"{}.tgt\".format(split), \"w\") as fout:\n",
    "        entries = data[split]\n",
    "        word_lists = []\n",
    "        for nodes in entries:\n",
    "            words = [node.word.strip() for node in nodes]\n",
    "            word_lists.append(words)\n",
    "            line = \" \".join(words)\n",
    "            fout.write(line + \"\\n\")\n",
    "    if split != \"train\":\n",
    "        # the pkl keeps the word and phrase boundary, useful in constrained decoding\n",
    "        with open(out / \"{}.tgt.pkl\".format(split), \"wb\") as fout:\n",
    "            pickle.dump(word_lists, fout)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def write_src_without_repeat(trees, path_out, get_line, uniqs):\n",
    "    # avoid duplicated source permutations with best effort try\n",
    "    with open(path_out, \"w\") as fout:\n",
    "        for tree in trees:\n",
    "            # resample if duplicate with limited retries\n",
    "            for _ in range(30):\n",
    "                line = get_line(tree)\n",
    "                if line not in uniqs:\n",
    "                    break\n",
    "            uniqs.add(line)\n",
    "            fout.write(line + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "dev\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "# base sources with different permutations for augmentations\n",
    "# corresponds to results in section 3.3 (main word ordering results)\n",
    "# and 3.5 (effects of input permutation)\n",
    "num_copy = 10\n",
    "mode = \"base\"\n",
    "param = mode2param[mode]\n",
    "uniqs = defaultdict(set)  # avoid duplicated input permutation for data augmentation setting\n",
    "\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    print(split)\n",
    "    entries = data[split]\n",
    "    trees = [partial_tree(nodes, param[\"keep_pos\"], param[\"keep_dep\"]) for nodes in entries]\n",
    "    for copy_id in range(0, num_copy + 1):\n",
    "        if copy_id == 0:\n",
    "            path_out = out / \"{}.{}.src\".format(mode, split)\n",
    "        else:\n",
    "            # additional\n",
    "            path_out = out / \"{}.{}.src.{}\".format(mode, split, copy_id)\n",
    "            if split == \"test\":\n",
    "                break\n",
    "\n",
    "\n",
    "        def get_line(tree):\n",
    "            return linearize(tree, bracket=param[\"bracket\"], shuffle=True, rel=param[\"rel\"])\n",
    "\n",
    "\n",
    "        write_src_without_repeat(trees, path_out, get_line, uniqs[split])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# sources with only PUNC to simulate unconditional generation\n",
    "# corresponds to results in section 3.5 (Effects of conditional modeling)\n",
    "source = \"<PUNCT:>\"\n",
    "mode = \"base\"\n",
    "param = mode2param[mode]\n",
    "for split in [\"train\", \"dev\", \"test\"]:\n",
    "    entries = data[split]\n",
    "    with open(out / \"u{}.{}.src\".format(mode, split), \"w\") as fout:\n",
    "        for _ in entries:\n",
    "            fout.write(source + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brac\n",
      "pos\n",
      "udep\n",
      "ldep\n",
      "full\n"
     ]
    }
   ],
   "source": [
    "# sources with different input features\n",
    "# corresponds to results in section 4 (understanding why BART helps)\n",
    "for mode, param in mode2param.items():\n",
    "    if mode == \"base\":\n",
    "        continue\n",
    "    print(mode)\n",
    "    for split in [\"train\", \"dev\", \"test\"]:\n",
    "        entries = data[split]\n",
    "        trees = [partial_tree(nodes, param[\"keep_pos\"], param[\"keep_dep\"]) for nodes in entries]\n",
    "        uniqs = set()  # keep the number of duplicate inputs minimal\n",
    "        path_out = out / \"{}.{}.src\".format(mode, split)\n",
    "\n",
    "\n",
    "        def get_line(tree):\n",
    "            return linearize(tree, bracket=param[\"bracket\"], shuffle=True, rel=param[\"rel\"])\n",
    "\n",
    "\n",
    "        write_src_without_repeat(trees, path_out, get_line, uniqs)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0.5\n",
      "0 1\n",
      "0.5 0\n",
      "0.5 0.5\n",
      "0.5 1\n",
      "1 0\n",
      "1 0.5\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "# sources for partial tree linearization\n",
    "# corresponds to results in section 5 (extension to partial tree linearization)\n",
    "for keep_pos, keep_dep in product([0, 0.5, 1], [0, 0.5, 1]):\n",
    "    print(keep_pos, keep_dep)\n",
    "    for split in [\"train\", \"dev\"]:\n",
    "        entries = data[split]\n",
    "        trees = [partial_tree(nodes, keep_pos, keep_dep) for nodes in entries]\n",
    "        uniqs = set()\n",
    "        path_out = out / \"{}.{}-{}.{}.src\".format(\"part\", keep_pos, keep_dep, split)\n",
    "\n",
    "\n",
    "        def get_line(tree):\n",
    "            return linearize(tree, bracket=True, shuffle=True, rel=True)\n",
    "\n",
    "\n",
    "        write_src_without_repeat(trees, path_out, get_line, uniqs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0.5\n",
      "0 1\n",
      "0.5 0\n",
      "0.5 0.5\n",
      "0.5 1\n",
      "1 0\n",
      "1 0.5\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "# sources for bnp partial tree linearization\n",
    "# corresponds to results in section 5 (extension to partial tree linearization)\n",
    "bnpdata = dict()\n",
    "for split in splits:\n",
    "    bnpdata[split] = parse_ptb_tree_data(data_path / \"{}.bnp.txt\".format(split))\n",
    "\n",
    "# targets constraints\n",
    "for split in [\"dev\"]:\n",
    "    entries = bnpdata[split]\n",
    "    word_lists = []\n",
    "    for nodes in entries:\n",
    "        words = [node.word.strip() for node in nodes]\n",
    "        word_lists.append(words)\n",
    "    with open(out / \"bnp.{}.tgt.pkl\".format(split), \"wb\") as fout:\n",
    "        # this pkl is used in constrained decoding.\n",
    "        # we need to keep the word and phrase boundary\n",
    "        pickle.dump(word_lists, fout)\n",
    "\n",
    "for keep_pos, keep_dep in product([0, 0.5, 1], [0, 0.5, 1]):\n",
    "    print(keep_pos, keep_dep)\n",
    "    for split in [\"train\", \"dev\"]:\n",
    "        entries = bnpdata[split]\n",
    "        trees = [partial_tree(nodes, keep_pos, keep_dep) for nodes in entries]\n",
    "        uniqs = set()\n",
    "        path_out = out / \"{}.{}-{}.{}.src\".format(\"bnp\", keep_pos, keep_dep, split)\n",
    "\n",
    "\n",
    "        def get_line(tree):\n",
    "            return linearize(tree, bracket=True, shuffle=True, rel=True)\n",
    "\n",
    "\n",
    "        write_src_without_repeat(trees, path_out, get_line, uniqs)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
