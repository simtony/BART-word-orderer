{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from itertools import chain, product\n",
    "from collections import namedtuple, defaultdict\n",
    "import sentencepiece as spm\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "raw = Path(\"data-raw\")\n",
    "rand = Path(\"data-rand\")\n",
    "bart = Path(\"data-bart\")\n",
    "os.makedirs(rand, exist_ok=True)\n",
    "os.makedirs(bart, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Train BPE tokenizer for RAND\n",
    "with open(raw/\"new_tokens.txt\", \"r\") as fin:\n",
    "    special_tokens = [line.strip() for line in fin]\n",
    "non_prefix = re.compile(\"({})\".format(\"|\".join(re.escape(i) for i in special_tokens)))\n",
    "\n",
    "def process_line(line):\n",
    "    line = \" \".join(line.strip().split())\n",
    "    if non_prefix.match(line):\n",
    "        # do not append space if the first token is special token\n",
    "        return line\n",
    "    else:\n",
    "        return \" \" + line\n",
    "\n",
    "# use the same bpe tokens for simplicity\n",
    "# all.train.src should contain all related tokens\n",
    "with open(raw/\"all.train.src\", \"r\") as fin:\n",
    "    spm.SentencePieceTrainer.train(sentence_iterator=map(process_line, fin),\n",
    "                                   model_prefix=str(rand/\"spm\"),\n",
    "                                   vocab_size=8000,\n",
    "                                   character_coverage=1.0,\n",
    "                                   model_type=\"bpe\",\n",
    "                                   split_by_whitespace=True,\n",
    "                                   user_defined_symbols=\",\".join(special_tokens),\n",
    "                                   add_dummy_prefix=False,\n",
    "                                   remove_extra_whitespaces=False)\n",
    "sp = spm.SentencePieceProcessor(model_file=str(rand/\"spm.model\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-raw/bnp.0-0.dev.src\n",
      "data-raw/bnp.0-1.train.src\n",
      "data-raw/bnp.0-0.5.dev.src\n",
      "data-raw/base.dev.src.5\n",
      "data-raw/base.dev.src.1\n",
      "data-raw/part.0.5-0.train.src\n",
      "data-raw/base.dev.src.4\n",
      "data-raw/full.test.src\n",
      "data-raw/part.1-0.5.dev.src\n",
      "data-raw/bnp.1-1.dev.src\n",
      "data-raw/base.dev.src\n",
      "data-raw/base.dev.src.8\n",
      "data-raw/bnp.1-0.5.dev.src\n",
      "data-raw/base.train.src.1\n",
      "data-raw/base.train.src\n",
      "data-raw/base.dev.src.7\n",
      "data-raw/base.train.src.7\n",
      "data-raw/base.train.src.2\n",
      "data-raw/ldep.train.src\n",
      "data-raw/part.0.5-0.5.dev.src\n",
      "data-raw/part.1-1.train.src\n",
      "data-raw/part.1-1.dev.src\n",
      "data-raw/udep.train.src\n",
      "data-raw/base.train.src.5\n",
      "data-raw/base.train.src.8\n",
      "data-raw/bnp.0-0.5.train.src\n",
      "data-raw/ldep.dev.src\n",
      "data-raw/ubase.test.src\n",
      "data-raw/bnp.0.5-0.dev.src\n",
      "data-raw/brac.test.src\n",
      "data-raw/base.train.src.4\n",
      "data-raw/part.0.5-0.5.train.src\n",
      "data-raw/base.train.src.9\n",
      "data-raw/base.test.src\n",
      "data-raw/bnp.1-1.train.src\n",
      "data-raw/udep.dev.src\n",
      "data-raw/base.train.src.6\n",
      "data-raw/part.1-0.dev.src\n",
      "data-raw/bnp.0.5-0.5.train.src\n",
      "data-raw/part.0-1.train.src\n",
      "data-raw/part.0-0.5.dev.src\n",
      "data-raw/ldep.test.src\n",
      "data-raw/bnp.0-0.train.src\n",
      "data-raw/part.0.5-1.dev.src\n",
      "data-raw/udep.test.src\n",
      "data-raw/pos.dev.src\n",
      "data-raw/base.dev.src.3\n",
      "data-raw/bnp.1-0.train.src\n",
      "data-raw/base.train.src.10\n",
      "data-raw/base.dev.src.6\n",
      "data-raw/bnp.1-0.dev.src\n",
      "data-raw/bnp.0-1.dev.src\n",
      "data-raw/ubase.train.src\n",
      "data-raw/bnp.0.5-1.dev.src\n",
      "data-raw/part.0-1.dev.src\n",
      "data-raw/pos.test.src\n",
      "data-raw/bnp.0.5-0.5.dev.src\n",
      "data-raw/base.dev.src.10\n",
      "data-raw/bnp.1-0.5.train.src\n",
      "data-raw/part.0-0.5.train.src\n",
      "data-raw/base.dev.src.2\n",
      "data-raw/bnp.0.5-0.train.src\n",
      "data-raw/brac.dev.src\n",
      "data-raw/brac.train.src\n",
      "data-raw/part.0-0.dev.src\n",
      "data-raw/base.dev.src.9\n",
      "data-raw/bnp.0.5-1.train.src\n",
      "data-raw/base.train.src.3\n",
      "data-raw/pos.train.src\n",
      "data-raw/ubase.dev.src\n",
      "data-raw/part.0.5-0.dev.src\n",
      "data-raw/part.0-0.train.src\n",
      "data-raw/full.train.src\n",
      "data-raw/full.dev.src\n",
      "data-raw/part.1-0.5.train.src\n",
      "data-raw/part.0.5-1.train.src\n",
      "data-raw/part.1-0.train.src\n",
      "data-raw/dev.tgt\n",
      "data-raw/train.tgt\n",
      "data-raw/test.tgt\n"
     ]
    }
   ],
   "source": [
    "# Apply the tokenizer for RAND\n",
    "for file in chain(raw.glob(\"*.src*\"), raw.glob(\"*.tgt*\")):\n",
    "    if \"pkl\" in file.name:\n",
    "        continue\n",
    "    print(file)\n",
    "    with open(file, \"r\") as fin, open(rand/file.name, \"w\") as fout:\n",
    "        for line in fin:\n",
    "            fout.write(\" \".join(sp.encode(process_line(line), out_type=str)) + \"\\n\")\n",
    "\n",
    "for file in raw.glob(\"*.tgt.pkl\"):\n",
    "    with open(file, \"rb\") as fin, open(rand/file.name.replace(\"pkl\", \"const\"), \"w\") as fout:\n",
    "        word_lists = pickle.load(fin)\n",
    "        for words in word_lists:\n",
    "            line = \"\\t\".join(\" \".join(sp.encode(process_line(word), out_type=str)) for word in words)\n",
    "            fout.write(line + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Prepare BPE tokenizers for BART\n",
    "# download bart base and extract it to ./bart\n",
    "# https://dl.fbaipublicfiles.com/fairseq/models/bart.base.tar.gz\n",
    "# download these files to ./bart\n",
    "# https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\n",
    "# https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\n",
    "\n",
    "# update the bart vocabulary with our special tokens\n",
    "with open(raw/\"new_tokens.txt\", \"r\") as fin:\n",
    "    special_tokens = [line.strip() for line in fin]\n",
    "    with open(\"./bart/dict.txt\", \"r\") as fin, \\\n",
    "         open(bart/\"dict.txt\", \"w\") as fout:\n",
    "        for i, line in enumerate(fin):\n",
    "            idx, count = line.strip().split(\" \")\n",
    "            if int(count) == 0 and \"madeupword\" in idx and special_tokens:\n",
    "                entry = (special_tokens.pop(), 1)\n",
    "            else:\n",
    "                entry = (idx, count)\n",
    "            fout.write(\"{} {}\\n\".format(entry[0], entry[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-raw/bnp.0-0.dev.src\n",
      "data-raw/bnp.0-1.train.src\n",
      "data-raw/bnp.0-0.5.dev.src\n",
      "data-raw/base.dev.src.5\n",
      "data-raw/base.dev.src.1\n",
      "data-raw/part.0.5-0.train.src\n",
      "data-raw/base.dev.src.4\n",
      "data-raw/full.test.src\n",
      "data-raw/part.1-0.5.dev.src\n",
      "data-raw/bnp.1-1.dev.src\n",
      "data-raw/base.dev.src\n",
      "data-raw/base.dev.src.8\n",
      "data-raw/bnp.1-0.5.dev.src\n",
      "data-raw/base.train.src.1\n",
      "data-raw/base.train.src\n",
      "data-raw/base.dev.src.7\n",
      "data-raw/base.train.src.7\n",
      "data-raw/base.train.src.2\n",
      "data-raw/ldep.train.src\n",
      "data-raw/part.0.5-0.5.dev.src\n",
      "data-raw/part.1-1.train.src\n",
      "data-raw/part.1-1.dev.src\n",
      "data-raw/udep.train.src\n",
      "data-raw/base.train.src.5\n",
      "data-raw/base.train.src.8\n",
      "data-raw/bnp.0-0.5.train.src\n",
      "data-raw/ldep.dev.src\n",
      "data-raw/ubase.test.src\n",
      "data-raw/bnp.0.5-0.dev.src\n",
      "data-raw/brac.test.src\n",
      "data-raw/base.train.src.4\n",
      "data-raw/part.0.5-0.5.train.src\n",
      "data-raw/base.train.src.9\n",
      "data-raw/base.test.src\n",
      "data-raw/bnp.1-1.train.src\n",
      "data-raw/udep.dev.src\n",
      "data-raw/base.train.src.6\n",
      "data-raw/part.1-0.dev.src\n",
      "data-raw/bnp.0.5-0.5.train.src\n",
      "data-raw/part.0-1.train.src\n",
      "data-raw/part.0-0.5.dev.src\n",
      "data-raw/ldep.test.src\n",
      "data-raw/bnp.0-0.train.src\n",
      "data-raw/part.0.5-1.dev.src\n",
      "data-raw/udep.test.src\n",
      "data-raw/pos.dev.src\n",
      "data-raw/base.dev.src.3\n",
      "data-raw/bnp.1-0.train.src\n",
      "data-raw/base.train.src.10\n",
      "data-raw/base.dev.src.6\n",
      "data-raw/bnp.1-0.dev.src\n",
      "data-raw/bnp.0-1.dev.src\n",
      "data-raw/ubase.train.src\n",
      "data-raw/bnp.0.5-1.dev.src\n",
      "data-raw/part.0-1.dev.src\n",
      "data-raw/pos.test.src\n",
      "data-raw/bnp.0.5-0.5.dev.src\n",
      "data-raw/base.dev.src.10\n",
      "data-raw/bnp.1-0.5.train.src\n",
      "data-raw/part.0-0.5.train.src\n",
      "data-raw/base.dev.src.2\n",
      "data-raw/bnp.0.5-0.train.src\n",
      "data-raw/brac.dev.src\n",
      "data-raw/brac.train.src\n",
      "data-raw/part.0-0.dev.src\n",
      "data-raw/base.dev.src.9\n",
      "data-raw/bnp.0.5-1.train.src\n",
      "data-raw/base.train.src.3\n",
      "data-raw/pos.train.src\n",
      "data-raw/ubase.dev.src\n",
      "data-raw/part.0.5-0.dev.src\n",
      "data-raw/part.0-0.train.src\n",
      "data-raw/full.train.src\n",
      "data-raw/full.dev.src\n",
      "data-raw/part.1-0.5.train.src\n",
      "data-raw/part.0.5-1.train.src\n",
      "data-raw/part.1-0.train.src\n",
      "data-raw/dev.tgt\n",
      "data-raw/train.tgt\n",
      "data-raw/test.tgt\n"
     ]
    }
   ],
   "source": [
    "# Apply BART BPE tokenization\n",
    "# please chek the path to encoder.json and vocab.bpe!\n",
    "cmd = \"python -m examples.roberta.multiprocessing_bpe_encoder \" \\\n",
    "      \"--encoder-json bart/encoder.json \" \\\n",
    "      \"--vocab-bpe bart/vocab.bpe \" \\\n",
    "      \"--inputs {}  --outputs {} --workers 10 --keep-empty\"\n",
    "for file in chain(raw.glob(\"*.src*\"), raw.glob(\"*.tgt*\")):\n",
    "    if \"pkl\" in file.name:\n",
    "        continue\n",
    "    print(file)\n",
    "    os.system(cmd.format(file, str(bart / file.name)))\n",
    "\n",
    "for file in raw.glob(\"*.tgt.pkl\"):\n",
    "    with open(file, \"rb\") as fin, open(bart / file.name.replace(\"pkl\", \"const\"), \"w\") as fout:\n",
    "        word_lists = pickle.load(fin)\n",
    "        # as BART decodes with fairseq interactive mode, which takes raw text inputs,\n",
    "        # we use raw text for constraints.\n",
    "        for words in word_lists:\n",
    "            fout.write(\"\\t\".join(words) + \"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Settings to assemble datasets for each experiments\n",
    "settings = {}\n",
    "pair = namedtuple(\"Pair\", [\"src\", \"tgt\", \"const\"])\n",
    "\n",
    "# \"ubase\": unconstrained decoding with only BoW inputs\n",
    "for case in [\"base\", \"brac\", \"ubase\", \"pos\", \"udep\", \"ldep\", \"all\"]:\n",
    "    settings[case] = {\n",
    "        \"train\": [pair(src=[\"{}.train.src\".format(case)], tgt=[\"train.tgt\"], const=None)],\n",
    "        \"valid\": [pair(src=[\"{}.dev.src\".format(case)], tgt=[\"dev.tgt\"], const=[\"dev.tgt.const\"])],\n",
    "        \"test\": [pair(src=[\"{}.test.src\".format(case)], tgt=[\"test.tgt\"], const=[\"test.tgt.const\"])]\n",
    "    }\n",
    "\n",
    "for case in [\"base0\", \"base2\", \"base4\", \"base6\", \"base8\"]:\n",
    "    count = int(re.search(\"\\d+$\", case)[0])\n",
    "    config = {}\n",
    "    config[\"train\"] = [pair(\n",
    "        src=[\"base.train.src\"] + [\"base.train.src.{}\".format(i) for i in range(1, count + 1)],\n",
    "        tgt=[\"train.tgt\"] * (count + 1), const=None\n",
    "    )]\n",
    "    config[\"valid\"] = [pair(src=[\"base.dev.src\"], tgt=[\"dev.tgt\"], const=None)]\n",
    "    config[\"test\"] = [pair(src=[\"base.test.src\"], tgt=[\"test.tgt\"], const=[\"test.tgt.const\"])]\n",
    "    config[\"test\"] += [pair(src=[\"base.dev.src.{}\".format(i)], tgt=[\"dev.tgt\"], const=[\"dev.tgt.const\"]) for i in range(1, 11)]\n",
    "    settings[case] = config\n",
    "\n",
    "# base setting with all subword tokens shuffled\n",
    "settings[\"sbase\"] = settings[\"base0\"]\n",
    "\n",
    "indices = list(product([0, 0.5, 1], [0, 0.5, 1]))\n",
    "for case in [\"bnp\", \"part\"]:\n",
    "    config = {}\n",
    "    config[\"train\"] = [pair(src=[\"{}.{}-{}.train.src\".format(case, p, d) for p, d in indices],\n",
    "                           tgt=[\"train.tgt\" for _ in indices], const=None)]\n",
    "    config[\"valid\"] = [pair(src=[\"{}.{}-{}.dev.src\".format(case, p, d) for p, d in indices],\n",
    "                           tgt=[\"dev.tgt\" for _ in indices], const=None)]\n",
    "    config[\"test\"] = [pair(src=[\"{}.{}-{}.dev.src\".format(case, p, d)],\n",
    "                           tgt=[\"dev.tgt\"], const=[\"bnp.dev.tgt.const\" if case == \"bnp\" else \"dev.tgt.const\"])\n",
    "                      for p, d in indices]\n",
    "    settings[case] = config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "brac\n",
      "ubase\n",
      "pos\n",
      "udep\n",
      "ldep\n",
      "full\n",
      "base0\n",
      "base2\n",
      "base4\n",
      "base6\n",
      "base8\n",
      "sbase\n",
      "bnp\n",
      "part\n"
     ]
    }
   ],
   "source": [
    "# Assemble dataset for RAND (from scratch) training\n",
    "for case, setting in settings.items():\n",
    "    case_out = rand/case\n",
    "    case_raw = case_out/\"raw\"\n",
    "    os.makedirs(case_raw, exist_ok=True)\n",
    "    prefixes = defaultdict(list)\n",
    "    print(case)\n",
    "    for split, pairs in setting.items():\n",
    "        for i, pair in enumerate(pairs):\n",
    "            prefix = \"{}{}\".format(split, str(i) if i else \"\")\n",
    "            prefixes[split].append(str(case_raw/prefix))\n",
    "            src_in = \" \".join(str(rand/f) for f in pair.src)\n",
    "            src_out = case_raw/\"{}.src\".format(prefix)\n",
    "            os.system(\"cat {} > {}\".format(src_in, src_out))\n",
    "            if case == \"sbase\" and split != \"test\":\n",
    "                src_out_bak = case_raw/\"{}.src.bak\".format(prefix)\n",
    "                os.system(\"cp {} {}\".format(src_out, src_out_bak))\n",
    "                with open(src_out_bak, \"r\") as fin, open(src_out, \"w\") as fout:\n",
    "                    for line in fin:\n",
    "                        tokens = line.strip().split()\n",
    "                        random.shuffle(tokens)\n",
    "                        fout.write(\" \".join(tokens) + \"\\n\")\n",
    "                os.system(\"rm {}\".format(src_out_bak))\n",
    "\n",
    "            tgt_in = \" \".join(str(rand/f) for f in pair.tgt)\n",
    "            tgt_out = case_raw/\"{}.tgt\".format(prefix)\n",
    "            os.system(\"cat {} > {}\".format(tgt_in, tgt_out))\n",
    "\n",
    "            if pair.const is not None:\n",
    "                const_in = \" \".join(str(rand/f) for f in pair.const)\n",
    "                const_out = case_raw/\"{}.const\".format(prefix)\n",
    "                os.system(\"cat {} > {}\".format(const_in, const_out))\n",
    "                with open(src_out, \"r\") as sin, open(const_out, \"r\") as cin, \\\n",
    "                     open(case_out/\"{}.mix\".format(prefix), \"w\") as fout:\n",
    "                    for sline, cline in zip(sin, cin):\n",
    "                        fout.write(\"{}\\t{}\\n\".format(sline.strip(), cline.strip()))\n",
    "                os.system(\"rm {}\".format(const_out))\n",
    "    cmd = \"fairseq-preprocess --source-lang src --target-lang tgt --trainpref {} --validpref {} --testpref {} --destdir {} --workers 20 --joined-dictionary\".format(\n",
    "            \",\".join(prefixes[\"train\"]), \",\".join(prefixes[\"valid\"]), \",\".join(prefixes[\"test\"]), case_out\n",
    "    )\n",
    "    os.system(cmd)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base\n",
      "brac\n",
      "ubase\n",
      "pos\n",
      "udep\n",
      "ldep\n",
      "all\n",
      "base0\n",
      "base2\n",
      "base4\n",
      "base6\n",
      "base8\n",
      "sbase\n",
      "bnp\n",
      "part\n"
     ]
    }
   ],
   "source": [
    "# Assemble dataset for BART finetuning\n",
    "for case, setting in settings.items():\n",
    "    case_out = bart/case\n",
    "    case_raw = case_out/\"raw\"\n",
    "    os.makedirs(case_raw, exist_ok=True)\n",
    "    prefixes = defaultdict(list)\n",
    "    print(case)\n",
    "    for split, pairs in setting.items():\n",
    "        for i, pair in enumerate(pairs):\n",
    "            prefix = \"{}{}\".format(split, str(i) if i else \"\")\n",
    "            prefixes[split].append(str(case_raw/prefix))\n",
    "            src_in = \" \".join(str(bart/f) for f in pair.src)\n",
    "            src_out = case_raw/\"{}.src\".format(prefix)\n",
    "            os.system(\"cat {} > {}\".format(src_in, src_out))\n",
    "            if case == \"sbase\" and split != \"test\":\n",
    "                src_out_bak = case_raw/\"{}.src.bak\".format(prefix)\n",
    "                os.system(\"cp {} {}\".format(src_out, src_out_bak))\n",
    "                with open(src_out_bak, \"r\") as fin, open(src_out, \"w\") as fout:\n",
    "                    for line in fin:\n",
    "                        tokens = line.strip().split()\n",
    "                        random.shuffle(tokens)\n",
    "                        fout.write(\" \".join(tokens) + \"\\n\")\n",
    "                os.system(\"rm {}\".format(src_out_bak))\n",
    "\n",
    "            tgt_in = \" \".join(str(bart/f) for f in pair.tgt)\n",
    "            tgt_out = case_raw/\"{}.tgt\".format(prefix)\n",
    "            os.system(\"cat {} > {}\".format(tgt_in, tgt_out))\n",
    "\n",
    "            if pair.const is not None:\n",
    "                # since bpe encoder is specified in interactive mode, we use raw inputs.\n",
    "                const_in = \" \".join(str(bart/f) for f in pair.const)\n",
    "                const_out = case_raw/\"{}.const\".format(prefix)\n",
    "                os.system(\"cat {} > {}\".format(const_in, const_out))\n",
    "\n",
    "                raw_src_in = \" \".join(str(raw/f) for f in pair.src)\n",
    "                raw_src_out = case_raw/\"{}.rawsrc\".format(prefix)\n",
    "                os.system(\"cat {} > {}\".format(raw_src_in, raw_src_out))\n",
    "\n",
    "                with open(raw_src_out, \"r\") as sin, open(const_out, \"r\") as cin, \\\n",
    "                     open(case_out/\"{}.mix\".format(prefix), \"w\") as fout:\n",
    "                    for sline, cline in zip(sin, cin):\n",
    "                        fout.write(\"{}\\t{}\\n\".format(sline.strip(), cline.strip()))\n",
    "                os.system(\"rm {}\".format(const_out))\n",
    "                os.system(\"rm {}\".format(raw_src_out))\n",
    "    cmd = \"fairseq-preprocess --source-lang src --target-lang tgt --trainpref {} --validpref {} --testpref {}  --destdir {} --workers 20 --srcdict {} --tgtdict {}\".format(\n",
    "        \",\".join(prefixes[\"train\"]), \",\".join(prefixes[\"valid\"]), \",\".join(prefixes[\"test\"]), case_out, str(bart/\"dict.txt\"), str(bart/\"dict.txt\")\n",
    ")\n",
    "    os.system(cmd)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
