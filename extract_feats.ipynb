{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from fairseq.models.transformer import TransformerModel\n",
    "from fairseq import tasks\n",
    "from fairseq.utils import resolve_max_positions\n",
    "from torch_scatter import scatter_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def get_word_ids(indices, batch, tgt_dict):\n",
    "    word_ids = []\n",
    "    for index, sent in zip(indices, batch):\n",
    "        index = index.item()\n",
    "        i = -1\n",
    "        ids = []\n",
    "        for t in sent:\n",
    "            token = tgt_dict[t]\n",
    "            if \"▁\" in token or \"</s>\" == token:\n",
    "                i += 1\n",
    "            elif index == 491 and \"<unk>\" == token:\n",
    "                # a dirty fix as this unk token of validation set happens to start the sentence\n",
    "                i += 1\n",
    "            ids.append(i)\n",
    "        word_ids.append(ids)\n",
    "    return word_ids\n",
    "\n",
    "def write_hidden(fout, indices, word_ids, inner_states):\n",
    "    stack_hidden = torch.stack(inner_states, dim=2).transpose(0, 1)\n",
    "    word_hidden = scatter_mean(stack_hidden, word_ids, dim=1)\n",
    "    for index, h, ids in zip(indices, word_hidden, word_ids):\n",
    "        out_h = h[:ids[-1]]\n",
    "        fout[str(index.item())] = out_h.transpose(0, 1).float().cpu().numpy()\n",
    "\n",
    "def forward(model, b, tgt_dict, out):\n",
    "    src_tokens = b['net_input']['src_tokens'].cuda()\n",
    "    src_lengths = b['net_input']['src_lengths'].cuda()\n",
    "    target = b['target']\n",
    "    prev_output_tokens = b['net_input']['prev_output_tokens'].cuda()\n",
    "    ids = b['id']\n",
    "    encoder_out = model.encoder.forward(src_tokens, src_lengths)\n",
    "    decoder_out = model.decoder(prev_output_tokens, encoder_out=encoder_out, features_only=True,\n",
    "                                return_all_hiddens=True)\n",
    "    tgt_word_ids = torch.tensor(get_word_ids(ids, target, tgt_dict), dtype=torch.long, device=prev_output_tokens.device)\n",
    "    write_hidden(out, ids, tgt_word_ids, decoder_out[1][\"inner_states\"])\n",
    "\n",
    "def extract_rand_feats(mode, rand_init):\n",
    "    data = \"rand\"\n",
    "    output_path = \"output/Data_{}-Mode_{}\".format(data, mode)\n",
    "    data_path = 'data-{}/{}'.format(data, mode)\n",
    "    out_dir = \"feats/{}_{}\".format(data, mode + \"rand\" if rand_init else mode)\n",
    "    max_tokens = 65536\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    linearizer = TransformerModel.from_pretrained(\n",
    "            output_path,\n",
    "            checkpoint_file='checkpoint_avg.pt',\n",
    "            data_name_or_path=data_path,\n",
    "            constraints=\"unordered\")\n",
    "    if rand_init:\n",
    "        linearizer.models[0] = linearizer.models[0].build_model(linearizer.args, linearizer.task)\n",
    "\n",
    "    linearizer.half().cuda()\n",
    "\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        print(split)\n",
    "        linearizer.task.load_dataset(split)\n",
    "        biter = linearizer.task.get_batch_iterator(\n",
    "                dataset=linearizer.task.dataset(split),\n",
    "                max_tokens=max_tokens,\n",
    "                max_positions=resolve_max_positions(\n",
    "                        linearizer.task.max_positions(),\n",
    "                        linearizer.models[0].max_positions(),\n",
    "                        max_tokens)\n",
    "        ).next_epoch_itr(shuffle=False)\n",
    "        path = os.path.join(out_dir, \"{}.hdf5\".format(split))\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "        with torch.no_grad(), h5py.File(path, \"w\") as out:\n",
    "            for b in biter:\n",
    "                forward(linearizer.models[0], b, linearizer.task.target_dictionary, out)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "valid\n",
      "test\n",
      "train\n",
      "valid\n",
      "test\n",
      "train\n",
      "valid\n",
      "test\n",
      "train\n",
      "valid\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "for mode in [\"base\", \"pos\", \"udep\"]:\n",
    "    extract_rand_feats(mode, False)\n",
    "    torch.cuda.empty_cache()\n",
    "extract_rand_feats(\"base\", True)\n",
    "torch.cuda.empty_cache()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_word_ids(batch, tgt_dict, bpe_decoder):\n",
    "    word_ids = []\n",
    "    word_pat = re.compile(r\"(Ġ|</s>)\")\n",
    "    for sent in batch:\n",
    "        i = -1\n",
    "        ids = []\n",
    "        for t in sent:\n",
    "            token = tgt_dict[t]\n",
    "            if token.isdecimal() and int(token) in bpe_decoder:\n",
    "                token = bpe_decoder[int(token)]\n",
    "            if word_pat.match(token):\n",
    "                i += 1\n",
    "            ids.append(i)\n",
    "        word_ids.append(ids)\n",
    "    return word_ids\n",
    "\n",
    "def write_hidden(fout, indices, word_ids, inner_states):\n",
    "    stack_hidden = torch.stack(inner_states, dim=2).transpose(0, 1)\n",
    "    word_hidden = scatter_mean(stack_hidden, word_ids, dim=1)\n",
    "    for index, h, ids in zip(indices, word_hidden, word_ids):\n",
    "        out_h = h[:ids[-1]]\n",
    "        fout[str(index.item())] = out_h.transpose(0, 1).float().cpu().numpy()\n",
    "\n",
    "def forward(model, b, tgt_dict, bpe_decoder, out):\n",
    "    src_tokens = b['net_input']['src_tokens'].cuda()\n",
    "    src_lengths = b['net_input']['src_lengths'].cuda()\n",
    "    target = b['target']\n",
    "    prev_output_tokens = b['net_input']['prev_output_tokens'].cuda()\n",
    "    ids = b['id']\n",
    "    encoder_out = model.encoder.forward(src_tokens, src_lengths)\n",
    "    decoder_out = model.decoder(prev_output_tokens, encoder_out=encoder_out, features_only=True, return_all_hiddens=True)\n",
    "    inner_states = decoder_out[1][\"inner_states\"]\n",
    "    tgt_word_ids = torch.tensor(get_word_ids(target, tgt_dict, bpe_decoder), dtype=torch.long, device=prev_output_tokens.device)\n",
    "    write_hidden(out, ids, tgt_word_ids, inner_states)\n",
    "\n",
    "def extract_bart_feats(mode, rand_init):\n",
    "    data = \"bart\"\n",
    "    output_path = \"output/Data_{}-Mode_{}\".format(data, mode)\n",
    "    data_path = 'data-{}/{}'.format(data, mode)\n",
    "    out_dir = \"feats/{}_{}\".format(data, mode + \"rand\" if rand_init else mode)\n",
    "    max_tokens = 16384\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    linearizer = TransformerModel.from_pretrained(\n",
    "        output_path,\n",
    "        checkpoint_file=\"checkpoint_avg.pt\",\n",
    "        data_name_or_path=data_path,\n",
    "        constraints=\"unordered\",\n",
    "        bpe=\"gpt2\",\n",
    "        gpt2_encoder_json=\"bart/encoder.json\",\n",
    "        gpt2_vocab_bpe=\"bart/vocab.bpe\")\n",
    "\n",
    "    if rand_init:\n",
    "        linearizer.models[0].from_pretrained(\"bart/bart.base/\", bpe=\"gpt2\",\n",
    "            gpt2_encoder_json=\"bart/encoder.json\",\n",
    "            gpt2_vocab_bpe=\"bart/vocab.bpe\")\n",
    "    linearizer.half().cuda()\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        linearizer.task.load_dataset(split)\n",
    "        biter = linearizer.task.get_batch_iterator(\n",
    "                    dataset=linearizer.task.dataset(split),\n",
    "                    max_tokens=max_tokens,\n",
    "                    max_positions=resolve_max_positions(\n",
    "                        linearizer.task.max_positions(),\n",
    "                        linearizer.models[0].max_positions(),\n",
    "                        max_tokens)\n",
    "                    ).next_epoch_itr(shuffle=False)\n",
    "        path = os.path.join(out_dir, \"{}.hdf5\".format(split))\n",
    "        if os.path.exists(path):\n",
    "            os.remove(path)\n",
    "        with torch.no_grad(), h5py.File(path, \"w\") as out:\n",
    "            for b in biter:\n",
    "                forward(linearizer.models[0], b, linearizer.task.target_dictionary,\n",
    "                        linearizer.bpe.bpe.decoder, out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "\n",
    "for mode in [\"base\", \"pos\", \"udep\"]:\n",
    "    extract_bart_feats(mode, False)\n",
    "    torch.cuda.empty_cache()\n",
    "extract_bart_feats(\"base\", True)\n",
    "torch.cuda.empty_cache()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}